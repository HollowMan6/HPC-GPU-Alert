%!TEX root = ../Thesis.tex
\chapter{Discussion}
\label{chap:discussion}
\section{Findings}
Developing and implementing the monitoring system and alert service for GPU resource utilization in HPC clusters has garnered several vital insights, paving the way for further exploration and advancement in this domain.

One of the primary findings of this research is the effectiveness of the implemented algorithms in detecting inefficient GPU resource usage. By analyzing real-time monitoring data, the system can identify instances, where jobs are not effectively utilizing allocated GPU resources, potentially leading to performance bottlenecks or resource wastage. This capability is crucial for optimizing job scheduling and resource allocation in HPC environments, enhancing overall system efficiency and throughput.

Moreover, integrating GPU energy consumption data into the monitoring system offers valuable insights into the sustainability aspects of HPC operations. With increasing focus on energy efficiency and environmental sustainability, understanding and managing power consumption in HPC clusters is paramount. By monitoring GPU energy usage and identifying energy-intensive tasks or jobs, HPC users can implement strategies to reduce power consumption, lower operational costs, and minimize their environmental footprint.

Furthermore, the research's practical implications extend beyond HPC cluster management to various application domains, including AI development and scientific computing. The ability to monitor and analyze GPU resource utilization in real time provides researchers and practitioners with valuable insights into the performance of AI algorithms, computational simulations, and data analysis workflows. By optimizing GPU resource allocation and usage, HPC users can accelerate AI model training, improve scientific simulations, and drive innovation in various fields.

Additionally, we highlight the potential challenges and limitations we have tackled in the monitoring system and alert service for this master's thesis, such as scalability issues with large-scale HPC clusters and compatibility with different hardware configurations (Nvidia/AMD).

\section{Related work}

Several job monitoring platforms for HPC have emerged in recent years, including Ganglia \cite{MASSIE2004817}, TACC Stats \cite{7081222}, XDMoD \cite{7106398}, LIKWID \cite{8049016}, LDMS \cite{10.1145/3225058.3225086}, PIKA \cite{9229636}, and MAP \cite{9229613,9556031}. However, these platforms lack GPU monitoring support, and few have real-time alerting and history visualization features.

Other notable works in this area use the Prometheus monitoring framework and the Grafana visualization toolkit, including Jobstats \cite{10.1145/3569951.3604396} and the work down by Jaelyn et al. \cite{10.1145/3569951.3597554}. These platforms are designed for both CPU and GPU clusters, and they leverage the Prometheus monitoring framework \cite{208870} and the Grafana visualization toolkit \cite{Chakraborty2021} to provide job-level information on CPU/GPU efficiencies and CPU/GPU memory usage. However, these out-of-the-box solutions operate with high-level APIs, and it is hard to access the streaming raw data for alert customization, such as machine learning algorithms to identify jobs in real-time. Performance can also be an issue, and it is tough to debug if something goes wrong when we implement these solutions to pre-exascale supercomputers such as LUMI.
